{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq3dq0H3WS_q",
        "outputId": "9b5a5254-85ce-4abb-a4df-4364c14d0861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zos0L2V-VvhK"
      },
      "source": [
        "# TP 6 : feedforward neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X92BDNPtVvhQ"
      },
      "source": [
        "Ce TP sera noté:\n",
        "- le notebook jupiter complet `tp6.ipybn` doit être rendu sur moodle au plus tard le dimanche 20 février à 20h,\n",
        "- le TP est à faire tout seul ou en binôme, il est strictement interdit de travailler en plus de deux sur un même fichier, \n",
        "- éviter de chercher du code sur internet, **toute source extérieure doit être explicitement mentionnée**, \n",
        "- tout plagiat avéré sera sanctionné. \n",
        "  \n",
        "Une soutenance par binôme en distanciel sera organisée la semaine du 21 au 25 février par rendez-vous. \n",
        "La note sera sur cinq niveaux (A, B, C, D, E): **il ne s'agit pas du projet final du cours** (ce dernier sera donné plus tard). La notation de ce TP jouera un rôle très relatif sur la notation finale de l'UE: son but est surtout de vous donner des retours individuels sur votre avancement. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pKLOQaHVvhR"
      },
      "source": [
        "Identifiants du binôme:\n",
        "\n",
        "  1. membre du groupe:\n",
        "   - NOM: HENNI                  \n",
        "   - PRÉNOM: Amar            \n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_U7-ertVvhS"
      },
      "source": [
        "## Réchauffement avec le TP 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66dddAp6VvhS"
      },
      "source": [
        "Dans cette section, nous reprenons le jeu de données des exercices 9 et 10 du TP3 et nous allons construire un réseau à deux couches qui classifie ces données.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3z9OB_pVvhT"
      },
      "source": [
        "### Exercice 1 (préparation du jeu de données)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JJ653NqVvhT"
      },
      "source": [
        "Téléchargez depuis moodle le fichier ```database.pt``` contenant un jeu de données de 3000 points dans la region $[-1,1]^2$ du plan cartésien, repartis en trois classes $0$, $1$ et $2$. Affichez ce jeu de données en récupérant le code nécessaire depuis le TP3 pour visualiser les points avec la fonction ```show_data```. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "u7kl98tCrjE8"
      },
      "outputs": [],
      "source": [
        "# visualisation data and model\n",
        "def show_data(data_inputs, data_labels, model=None):\n",
        "    zeros = ([x[0] for x,l in zip(data_inputs, data_labels) if l == 0], [x[1] for x,l in zip(data_inputs, data_labels) if l == 0])\n",
        "    ones = ([x[0] for x,l in zip(data_inputs, data_labels) if l == 1], [x[1] for x,l in zip(data_inputs, data_labels) if l == 1])\n",
        "    twos = ([x[0] for x,l in zip(data_inputs, data_labels) if l == 2], [x[1] for x,l in zip(data_inputs, data_labels) if l == 2])\n",
        "    fig, ax = plt.subplots()\n",
        "    if model is not None: # for plotting decision boundary of the model\n",
        "        step = 0.02\n",
        "        xx, yy = torch.meshgrid(torch.arange(-1.0, 1.1, step), torch.arange(-1.0, 1.1, step), indexing='ij')\n",
        "        labels = model(torch.column_stack((xx.ravel(),yy.ravel())))\n",
        "        labels = (labels > 0.5)\n",
        "        plt.pcolormesh(xx, yy, labels.reshape(xx.shape), alpha=0.7, shading='auto')\n",
        "    ax.scatter(twos[0], twos[1], c='green', edgecolor=\"white\", linewidth=1)\n",
        "    ax.scatter(ones[0], ones[1], c='blue', edgecolor=\"white\", linewidth=1)\n",
        "    ax.scatter(zeros[0], zeros[1], c='red', edgecolor=\"white\", linewidth=1)        \n",
        "    plt.show()\n",
        "  \n",
        "def show_learning(history):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot([h['epoch'] for h in history], [h['loss'] for h in history], label='loss')\n",
        "  ax.plot([h['epoch'] for h in history], [h['train_acc'] for h in history], label='train accuracy')\n",
        "  ax.plot([h['epoch'] for h in history], [h['validation_acc'] for h in history], label='test accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "D0O-MOVKVvhU",
        "outputId": "41a8a320-5511-446d-adad-ab4b63991c0c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# STUDENT, THIS IS YOUR JOB!\n",
        "data_inputs, data_labels = torch.load('./drive/MyDrive/data/database.pt')\n",
        "show_data(data_inputs,data_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNVAppI7VvhV"
      },
      "source": [
        "Pour être sur que les échantillons ont une distribution uniforme sur des tranches, mélangez le jeu de données en utilisant la fonction `torch.randperm`, ensuite séparez ce jeu en \"training data-set\" et \"validation data-set\", avec une répartition de, respectivement, 80% de données pour le premier et 20% de données pour le deuxième. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ot58HW6mVvhW"
      },
      "outputs": [],
      "source": [
        "# STUDENT, THIS IS YOUR JOB!\n",
        "permutation = torch.randperm(len(data_inputs))\n",
        "data_inputs, data_labels = data_inputs[permutation], data_labels[permutation]\n",
        "# now I can split\n",
        "split = 8 * (len(data_inputs)) // 10 # 80% training, 20% validation\n",
        "train_inputs = data_inputs[:split]\n",
        "train_labels = data_labels[:split]\n",
        "validation_inputs = data_inputs[split:]\n",
        "validation_labels = data_labels[split:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RQTjAetVvhX"
      },
      "source": [
        "### Exercice 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un0cKMdGVvhX"
      },
      "source": [
        "Définissez un réseau de neurones avec deux couches (layer en anglais), à vous de choisir de bonnes fonctions d'activation. Faites apprendre ce réseau sur le jeu de données ci-dessus. Quelle fonction de perte pensez vous d'utiliser ? Trouvez les bons paramètres pour obtenir un taux d'exactitude sur le \"validation data-set\" supérieur à 99%. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUJEp_BkVvhX",
        "outputId": "ac8230e2-f3eb-4373-f158-69c7f6946282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4275, 0.3256, 0.2469],\n",
            "        [0.3880, 0.3507, 0.2613],\n",
            "        [0.4272, 0.3258, 0.2470],\n",
            "        [0.3781, 0.3569, 0.2650],\n",
            "        [0.4328, 0.3189, 0.2483]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# STUDENT, THIS IS YOUR JOB !\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,num_features):\n",
        "        super(FeedForward, self).__init__()                     \n",
        "        self.fct1 = Linear(num_features, (num_features*3)**2, bias=True)  \n",
        "        self.fct2 = Linear((num_features*3)**2, 3, bias=True)  \n",
        "    \n",
        "    def forward(self, input):\n",
        "      x1 = self.fct1(input)\n",
        "      layer1 = F.relu(x1)\n",
        "      x2 = self.fct2(layer1)\n",
        "      output = F.softmax(x2, dim=1)\n",
        "      return output\n",
        "\n",
        "model = FeedForward(2)\n",
        "print(model(train_inputs[:5])) # inputs [(x,y),...] -> [[distribution uniforme]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "v4HiuHG5ydiI"
      },
      "outputs": [],
      "source": [
        "def learning_process(model, loss_function, lr=1, n_epochs=200, accuracy=None):\n",
        "    history = []    # to monitor learning \n",
        "    for epoch in range(n_epochs):    \n",
        "        predictions = model(train_inputs)\n",
        "        loss = loss_function(predictions, train_labels)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                p -= lr*p.grad\n",
        "\n",
        "            if epoch % 10 == 0: # to monitor learning\n",
        "                train_acc = accuracy(predictions, train_labels) \n",
        "                validation_acc = accuracy(model(validation_inputs), validation_labels) \n",
        "                history.append({\n",
        "                    'epoch':epoch, \n",
        "                    'loss':loss.detach().numpy(), \n",
        "                    'train_acc': train_acc, \n",
        "                    'validation_acc': validation_acc\n",
        "                })\n",
        "                print(f\"epoch {epoch+1}/{n_epochs},\\t loss = {loss.detach().numpy()}\")        \n",
        "    return history\n",
        "  \n",
        "def accuracy(predictions, labels):\n",
        "  # PREDICTION IS TRIPLET\n",
        "  predictions = predictions.argmax(1) # exemple : [0.3, 0.2, 0.5] -> 2\n",
        "  n_correct = (predictions == labels).sum().detach().numpy()\n",
        "  return n_correct / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uAZM3ml0bIx",
        "outputId": "1beb2732-eb39-438c-80d0-7eac6069444f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/5000,\t loss = 1.0862548351287842\n",
            "epoch 11/5000,\t loss = 1.0344587564468384\n",
            "epoch 21/5000,\t loss = 0.9961158037185669\n",
            "epoch 31/5000,\t loss = 0.9681355953216553\n",
            "epoch 41/5000,\t loss = 0.9476594924926758\n",
            "epoch 51/5000,\t loss = 0.9329683184623718\n",
            "epoch 61/5000,\t loss = 0.9224965572357178\n",
            "epoch 71/5000,\t loss = 0.9148647785186768\n",
            "epoch 81/5000,\t loss = 0.9090773463249207\n",
            "epoch 91/5000,\t loss = 0.9044638276100159\n",
            "epoch 101/5000,\t loss = 0.900602400302887\n",
            "epoch 111/5000,\t loss = 0.8972437381744385\n",
            "epoch 121/5000,\t loss = 0.8941935300827026\n",
            "epoch 131/5000,\t loss = 0.8913808465003967\n",
            "epoch 141/5000,\t loss = 0.8887457251548767\n",
            "epoch 151/5000,\t loss = 0.8862045407295227\n",
            "epoch 161/5000,\t loss = 0.8837071657180786\n",
            "epoch 171/5000,\t loss = 0.8812192678451538\n",
            "epoch 181/5000,\t loss = 0.8787169456481934\n",
            "epoch 191/5000,\t loss = 0.8761757612228394\n",
            "epoch 201/5000,\t loss = 0.8735901713371277\n",
            "epoch 211/5000,\t loss = 0.8709418773651123\n",
            "epoch 221/5000,\t loss = 0.8682237863540649\n",
            "epoch 231/5000,\t loss = 0.8654281497001648\n",
            "epoch 241/5000,\t loss = 0.8625549077987671\n",
            "epoch 251/5000,\t loss = 0.8596099615097046\n",
            "epoch 261/5000,\t loss = 0.856596052646637\n",
            "epoch 271/5000,\t loss = 0.853525698184967\n",
            "epoch 281/5000,\t loss = 0.8504051566123962\n",
            "epoch 291/5000,\t loss = 0.847246527671814\n",
            "epoch 301/5000,\t loss = 0.8440624475479126\n",
            "epoch 311/5000,\t loss = 0.8408631682395935\n",
            "epoch 321/5000,\t loss = 0.8376604914665222\n",
            "epoch 331/5000,\t loss = 0.8344756364822388\n",
            "epoch 341/5000,\t loss = 0.8313199877738953\n",
            "epoch 351/5000,\t loss = 0.8282094597816467\n",
            "epoch 361/5000,\t loss = 0.8251527547836304\n",
            "epoch 371/5000,\t loss = 0.8221593499183655\n",
            "epoch 381/5000,\t loss = 0.8192412257194519\n",
            "epoch 391/5000,\t loss = 0.8164032697677612\n",
            "epoch 401/5000,\t loss = 0.8136488199234009\n",
            "epoch 411/5000,\t loss = 0.8109818696975708\n",
            "epoch 421/5000,\t loss = 0.8084024786949158\n",
            "epoch 431/5000,\t loss = 0.8059113621711731\n",
            "epoch 441/5000,\t loss = 0.8035089373588562\n",
            "epoch 451/5000,\t loss = 0.8011942505836487\n",
            "epoch 461/5000,\t loss = 0.7989630699157715\n",
            "epoch 471/5000,\t loss = 0.7968141436576843\n",
            "epoch 481/5000,\t loss = 0.7947463393211365\n",
            "epoch 491/5000,\t loss = 0.7927579283714294\n",
            "epoch 501/5000,\t loss = 0.7908440232276917\n",
            "epoch 511/5000,\t loss = 0.7890000939369202\n",
            "epoch 521/5000,\t loss = 0.7872230410575867\n",
            "epoch 531/5000,\t loss = 0.7855092883110046\n",
            "epoch 541/5000,\t loss = 0.7838548421859741\n",
            "epoch 551/5000,\t loss = 0.7822592854499817\n",
            "epoch 561/5000,\t loss = 0.7807193994522095\n",
            "epoch 571/5000,\t loss = 0.7792348861694336\n",
            "epoch 581/5000,\t loss = 0.7778003811836243\n",
            "epoch 591/5000,\t loss = 0.7764156460762024\n",
            "epoch 601/5000,\t loss = 0.7750759720802307\n",
            "epoch 611/5000,\t loss = 0.7737776041030884\n",
            "epoch 621/5000,\t loss = 0.772521436214447\n",
            "epoch 631/5000,\t loss = 0.7713029384613037\n",
            "epoch 641/5000,\t loss = 0.7701204419136047\n",
            "epoch 651/5000,\t loss = 0.7689715027809143\n",
            "epoch 661/5000,\t loss = 0.7678574919700623\n",
            "epoch 671/5000,\t loss = 0.7667765617370605\n",
            "epoch 681/5000,\t loss = 0.7657251954078674\n",
            "epoch 691/5000,\t loss = 0.7646998763084412\n",
            "epoch 701/5000,\t loss = 0.763700008392334\n",
            "epoch 711/5000,\t loss = 0.7627258896827698\n",
            "epoch 721/5000,\t loss = 0.761776864528656\n",
            "epoch 731/5000,\t loss = 0.7608497142791748\n",
            "epoch 741/5000,\t loss = 0.7599422931671143\n",
            "epoch 751/5000,\t loss = 0.7590541839599609\n",
            "epoch 761/5000,\t loss = 0.7581837773323059\n",
            "epoch 771/5000,\t loss = 0.7573309540748596\n",
            "epoch 781/5000,\t loss = 0.7564933896064758\n",
            "epoch 791/5000,\t loss = 0.7556707262992859\n",
            "epoch 801/5000,\t loss = 0.754863977432251\n",
            "epoch 811/5000,\t loss = 0.7540701031684875\n",
            "epoch 821/5000,\t loss = 0.7532890439033508\n",
            "epoch 831/5000,\t loss = 0.7525216937065125\n",
            "epoch 841/5000,\t loss = 0.7517673373222351\n",
            "epoch 851/5000,\t loss = 0.7510234117507935\n",
            "epoch 861/5000,\t loss = 0.7502906918525696\n",
            "epoch 871/5000,\t loss = 0.7495726943016052\n",
            "epoch 881/5000,\t loss = 0.7488664984703064\n",
            "epoch 891/5000,\t loss = 0.7481712698936462\n",
            "epoch 901/5000,\t loss = 0.7474863529205322\n",
            "epoch 911/5000,\t loss = 0.7468103766441345\n",
            "epoch 921/5000,\t loss = 0.7461428642272949\n",
            "epoch 931/5000,\t loss = 0.7454854249954224\n",
            "epoch 941/5000,\t loss = 0.7448367476463318\n",
            "epoch 951/5000,\t loss = 0.7441976070404053\n",
            "epoch 961/5000,\t loss = 0.7435649037361145\n",
            "epoch 971/5000,\t loss = 0.7429395318031311\n",
            "epoch 981/5000,\t loss = 0.7423235177993774\n",
            "epoch 991/5000,\t loss = 0.7417154312133789\n",
            "epoch 1001/5000,\t loss = 0.7411150336265564\n",
            "epoch 1011/5000,\t loss = 0.7405227422714233\n",
            "epoch 1021/5000,\t loss = 0.7399389743804932\n",
            "epoch 1031/5000,\t loss = 0.7393602728843689\n",
            "epoch 1041/5000,\t loss = 0.7387875318527222\n",
            "epoch 1051/5000,\t loss = 0.7382217645645142\n",
            "epoch 1061/5000,\t loss = 0.7376642227172852\n",
            "epoch 1071/5000,\t loss = 0.7371151447296143\n",
            "epoch 1081/5000,\t loss = 0.7365731000900269\n",
            "epoch 1091/5000,\t loss = 0.7360374927520752\n",
            "epoch 1101/5000,\t loss = 0.7355087399482727\n",
            "epoch 1111/5000,\t loss = 0.7349865436553955\n",
            "epoch 1121/5000,\t loss = 0.7344687581062317\n",
            "epoch 1131/5000,\t loss = 0.7339557409286499\n",
            "epoch 1141/5000,\t loss = 0.7334484457969666\n",
            "epoch 1151/5000,\t loss = 0.7329463958740234\n",
            "epoch 1161/5000,\t loss = 0.7324497699737549\n",
            "epoch 1171/5000,\t loss = 0.7319583892822266\n",
            "epoch 1181/5000,\t loss = 0.7314726710319519\n",
            "epoch 1191/5000,\t loss = 0.7309916019439697\n",
            "epoch 1201/5000,\t loss = 0.7305155396461487\n",
            "epoch 1211/5000,\t loss = 0.7300443053245544\n",
            "epoch 1221/5000,\t loss = 0.7295773029327393\n",
            "epoch 1231/5000,\t loss = 0.7291151285171509\n",
            "epoch 1241/5000,\t loss = 0.7286568880081177\n",
            "epoch 1251/5000,\t loss = 0.7282029986381531\n",
            "epoch 1261/5000,\t loss = 0.7277528643608093\n",
            "epoch 1271/5000,\t loss = 0.7273069024085999\n",
            "epoch 1281/5000,\t loss = 0.7268655300140381\n",
            "epoch 1291/5000,\t loss = 0.7264270782470703\n",
            "epoch 1301/5000,\t loss = 0.7259920835494995\n",
            "epoch 1311/5000,\t loss = 0.7255597114562988\n",
            "epoch 1321/5000,\t loss = 0.7251314520835876\n",
            "epoch 1331/5000,\t loss = 0.7247070074081421\n",
            "epoch 1341/5000,\t loss = 0.724285900592804\n",
            "epoch 1351/5000,\t loss = 0.7238683700561523\n",
            "epoch 1361/5000,\t loss = 0.7234529852867126\n",
            "epoch 1371/5000,\t loss = 0.7230393886566162\n",
            "epoch 1381/5000,\t loss = 0.7226282358169556\n",
            "epoch 1391/5000,\t loss = 0.7222196459770203\n",
            "epoch 1401/5000,\t loss = 0.721813440322876\n",
            "epoch 1411/5000,\t loss = 0.7214102745056152\n",
            "epoch 1421/5000,\t loss = 0.7210075855255127\n",
            "epoch 1431/5000,\t loss = 0.7206072807312012\n",
            "epoch 1441/5000,\t loss = 0.720209002494812\n",
            "epoch 1451/5000,\t loss = 0.7198113799095154\n",
            "epoch 1461/5000,\t loss = 0.7194135189056396\n",
            "epoch 1471/5000,\t loss = 0.7190151214599609\n",
            "epoch 1481/5000,\t loss = 0.7186183929443359\n",
            "epoch 1491/5000,\t loss = 0.7182224988937378\n",
            "epoch 1501/5000,\t loss = 0.7178264856338501\n",
            "epoch 1511/5000,\t loss = 0.7174308896064758\n",
            "epoch 1521/5000,\t loss = 0.7170328497886658\n",
            "epoch 1531/5000,\t loss = 0.7166363596916199\n",
            "epoch 1541/5000,\t loss = 0.7162399291992188\n",
            "epoch 1551/5000,\t loss = 0.7158430814743042\n",
            "epoch 1561/5000,\t loss = 0.7154452800750732\n",
            "epoch 1571/5000,\t loss = 0.715045690536499\n",
            "epoch 1581/5000,\t loss = 0.7146456837654114\n",
            "epoch 1591/5000,\t loss = 0.7142447233200073\n",
            "epoch 1601/5000,\t loss = 0.713843047618866\n",
            "epoch 1611/5000,\t loss = 0.7134390473365784\n",
            "epoch 1621/5000,\t loss = 0.7130352854728699\n",
            "epoch 1631/5000,\t loss = 0.7126283049583435\n",
            "epoch 1641/5000,\t loss = 0.7122190594673157\n",
            "epoch 1651/5000,\t loss = 0.7118076086044312\n",
            "epoch 1661/5000,\t loss = 0.7113962173461914\n",
            "epoch 1671/5000,\t loss = 0.7109846472740173\n",
            "epoch 1681/5000,\t loss = 0.7105686664581299\n",
            "epoch 1691/5000,\t loss = 0.7101473808288574\n",
            "epoch 1701/5000,\t loss = 0.7097166180610657\n",
            "epoch 1711/5000,\t loss = 0.7092753052711487\n",
            "epoch 1721/5000,\t loss = 0.7088229656219482\n",
            "epoch 1731/5000,\t loss = 0.7083560824394226\n",
            "epoch 1741/5000,\t loss = 0.7078737616539001\n",
            "epoch 1751/5000,\t loss = 0.7073767185211182\n",
            "epoch 1761/5000,\t loss = 0.7068549990653992\n",
            "epoch 1771/5000,\t loss = 0.7062981724739075\n",
            "epoch 1781/5000,\t loss = 0.7057129740715027\n",
            "epoch 1791/5000,\t loss = 0.7050814032554626\n",
            "epoch 1801/5000,\t loss = 0.704412043094635\n",
            "epoch 1811/5000,\t loss = 0.7036974430084229\n",
            "epoch 1821/5000,\t loss = 0.7029322385787964\n",
            "epoch 1831/5000,\t loss = 0.7021278738975525\n",
            "epoch 1841/5000,\t loss = 0.7013010382652283\n",
            "epoch 1851/5000,\t loss = 0.7004315853118896\n",
            "epoch 1861/5000,\t loss = 0.699535071849823\n",
            "epoch 1871/5000,\t loss = 0.6986240744590759\n",
            "epoch 1881/5000,\t loss = 0.6976980566978455\n",
            "epoch 1891/5000,\t loss = 0.6967592835426331\n",
            "epoch 1901/5000,\t loss = 0.6958096027374268\n",
            "epoch 1911/5000,\t loss = 0.6948726177215576\n",
            "epoch 1921/5000,\t loss = 0.6939407587051392\n",
            "epoch 1931/5000,\t loss = 0.6930024027824402\n",
            "epoch 1941/5000,\t loss = 0.6920469999313354\n",
            "epoch 1951/5000,\t loss = 0.6910756230354309\n",
            "epoch 1961/5000,\t loss = 0.6900811791419983\n",
            "epoch 1971/5000,\t loss = 0.6890393495559692\n",
            "epoch 1981/5000,\t loss = 0.6879420280456543\n",
            "epoch 1991/5000,\t loss = 0.6867930889129639\n",
            "epoch 2001/5000,\t loss = 0.6855795979499817\n",
            "epoch 2011/5000,\t loss = 0.6842812895774841\n",
            "epoch 2021/5000,\t loss = 0.6828532218933105\n",
            "epoch 2031/5000,\t loss = 0.6812789440155029\n",
            "epoch 2041/5000,\t loss = 0.6795326471328735\n",
            "epoch 2051/5000,\t loss = 0.677634060382843\n",
            "epoch 2061/5000,\t loss = 0.6756809949874878\n",
            "epoch 2071/5000,\t loss = 0.6737852096557617\n",
            "epoch 2081/5000,\t loss = 0.6720086932182312\n",
            "epoch 2091/5000,\t loss = 0.6703605651855469\n",
            "epoch 2101/5000,\t loss = 0.6688352227210999\n",
            "epoch 2111/5000,\t loss = 0.6674095392227173\n",
            "epoch 2121/5000,\t loss = 0.6660563945770264\n",
            "epoch 2131/5000,\t loss = 0.664772093296051\n",
            "epoch 2141/5000,\t loss = 0.6635399460792542\n",
            "epoch 2151/5000,\t loss = 0.6623414158821106\n",
            "epoch 2161/5000,\t loss = 0.661169171333313\n",
            "epoch 2171/5000,\t loss = 0.6600316762924194\n",
            "epoch 2181/5000,\t loss = 0.6589264273643494\n",
            "epoch 2191/5000,\t loss = 0.6578470468521118\n",
            "epoch 2201/5000,\t loss = 0.6568012833595276\n",
            "epoch 2211/5000,\t loss = 0.6557914614677429\n",
            "epoch 2221/5000,\t loss = 0.6548255085945129\n",
            "epoch 2231/5000,\t loss = 0.6539031863212585\n",
            "epoch 2241/5000,\t loss = 0.6530155539512634\n",
            "epoch 2251/5000,\t loss = 0.6521673798561096\n",
            "epoch 2261/5000,\t loss = 0.6513631343841553\n",
            "epoch 2271/5000,\t loss = 0.6505991816520691\n",
            "epoch 2281/5000,\t loss = 0.6498685479164124\n",
            "epoch 2291/5000,\t loss = 0.6491702795028687\n",
            "epoch 2301/5000,\t loss = 0.6485036015510559\n",
            "epoch 2311/5000,\t loss = 0.6478546857833862\n",
            "epoch 2321/5000,\t loss = 0.6472294330596924\n",
            "epoch 2331/5000,\t loss = 0.6466250419616699\n",
            "epoch 2341/5000,\t loss = 0.6460413336753845\n",
            "epoch 2351/5000,\t loss = 0.6454790234565735\n",
            "epoch 2361/5000,\t loss = 0.6449340581893921\n",
            "epoch 2371/5000,\t loss = 0.6444066166877747\n",
            "epoch 2381/5000,\t loss = 0.6438942551612854\n",
            "epoch 2391/5000,\t loss = 0.643394947052002\n",
            "epoch 2401/5000,\t loss = 0.6429082751274109\n",
            "epoch 2411/5000,\t loss = 0.6424307227134705\n",
            "epoch 2421/5000,\t loss = 0.6419585943222046\n",
            "epoch 2431/5000,\t loss = 0.6414881944656372\n",
            "epoch 2441/5000,\t loss = 0.6410223245620728\n",
            "epoch 2451/5000,\t loss = 0.6405659317970276\n",
            "epoch 2461/5000,\t loss = 0.6401190757751465\n",
            "epoch 2471/5000,\t loss = 0.6396724581718445\n",
            "epoch 2481/5000,\t loss = 0.6392306089401245\n",
            "epoch 2491/5000,\t loss = 0.6387946009635925\n",
            "epoch 2501/5000,\t loss = 0.6383615732192993\n",
            "epoch 2511/5000,\t loss = 0.6379368901252747\n",
            "epoch 2521/5000,\t loss = 0.6375191807746887\n",
            "epoch 2531/5000,\t loss = 0.6371093392372131\n",
            "epoch 2541/5000,\t loss = 0.636702299118042\n",
            "epoch 2551/5000,\t loss = 0.6363000869750977\n",
            "epoch 2561/5000,\t loss = 0.6359043121337891\n",
            "epoch 2571/5000,\t loss = 0.6355154514312744\n",
            "epoch 2581/5000,\t loss = 0.6351325511932373\n",
            "epoch 2591/5000,\t loss = 0.6347548365592957\n",
            "epoch 2601/5000,\t loss = 0.6343798041343689\n",
            "epoch 2611/5000,\t loss = 0.6340088248252869\n",
            "epoch 2621/5000,\t loss = 0.6336381435394287\n",
            "epoch 2631/5000,\t loss = 0.6332720518112183\n",
            "epoch 2641/5000,\t loss = 0.6329106092453003\n",
            "epoch 2651/5000,\t loss = 0.6325533986091614\n",
            "epoch 2661/5000,\t loss = 0.6322007775306702\n",
            "epoch 2671/5000,\t loss = 0.6318520903587341\n",
            "epoch 2681/5000,\t loss = 0.631508469581604\n",
            "epoch 2691/5000,\t loss = 0.6311690211296082\n",
            "epoch 2701/5000,\t loss = 0.6308329701423645\n",
            "epoch 2711/5000,\t loss = 0.6304988265037537\n",
            "epoch 2721/5000,\t loss = 0.6301683783531189\n",
            "epoch 2731/5000,\t loss = 0.6298379302024841\n",
            "epoch 2741/5000,\t loss = 0.6295092701911926\n",
            "epoch 2751/5000,\t loss = 0.629184365272522\n",
            "epoch 2761/5000,\t loss = 0.6288638710975647\n",
            "epoch 2771/5000,\t loss = 0.628547728061676\n",
            "epoch 2781/5000,\t loss = 0.6282324194908142\n",
            "epoch 2791/5000,\t loss = 0.6279170513153076\n",
            "epoch 2801/5000,\t loss = 0.6276033520698547\n",
            "epoch 2811/5000,\t loss = 0.6272944211959839\n",
            "epoch 2821/5000,\t loss = 0.6269896030426025\n",
            "epoch 2831/5000,\t loss = 0.62668776512146\n",
            "epoch 2841/5000,\t loss = 0.6263867616653442\n",
            "epoch 2851/5000,\t loss = 0.6260892152786255\n",
            "epoch 2861/5000,\t loss = 0.6257937550544739\n",
            "epoch 2871/5000,\t loss = 0.6254998445510864\n",
            "epoch 2881/5000,\t loss = 0.6252100467681885\n",
            "epoch 2891/5000,\t loss = 0.6249237656593323\n",
            "epoch 2901/5000,\t loss = 0.6246397495269775\n",
            "epoch 2911/5000,\t loss = 0.6243588328361511\n",
            "epoch 2921/5000,\t loss = 0.6240789294242859\n",
            "epoch 2931/5000,\t loss = 0.6238011717796326\n",
            "epoch 2941/5000,\t loss = 0.6235242486000061\n",
            "epoch 2951/5000,\t loss = 0.6232503652572632\n",
            "epoch 2961/5000,\t loss = 0.6229796409606934\n",
            "epoch 2971/5000,\t loss = 0.6227117776870728\n",
            "epoch 2981/5000,\t loss = 0.6224462389945984\n",
            "epoch 2991/5000,\t loss = 0.6221839189529419\n",
            "epoch 3001/5000,\t loss = 0.6219240427017212\n",
            "epoch 3011/5000,\t loss = 0.6216668486595154\n",
            "epoch 3021/5000,\t loss = 0.6214122772216797\n",
            "epoch 3031/5000,\t loss = 0.6211599707603455\n",
            "epoch 3041/5000,\t loss = 0.6209103465080261\n",
            "epoch 3051/5000,\t loss = 0.6206629276275635\n",
            "epoch 3061/5000,\t loss = 0.6204163432121277\n",
            "epoch 3071/5000,\t loss = 0.6201668381690979\n",
            "epoch 3081/5000,\t loss = 0.6199192404747009\n",
            "epoch 3091/5000,\t loss = 0.6196754574775696\n",
            "epoch 3101/5000,\t loss = 0.6194342374801636\n",
            "epoch 3111/5000,\t loss = 0.6191946268081665\n",
            "epoch 3121/5000,\t loss = 0.6189554929733276\n",
            "epoch 3131/5000,\t loss = 0.6187190413475037\n",
            "epoch 3141/5000,\t loss = 0.618485152721405\n",
            "epoch 3151/5000,\t loss = 0.6182536482810974\n",
            "epoch 3161/5000,\t loss = 0.6180241703987122\n",
            "epoch 3171/5000,\t loss = 0.6177952289581299\n",
            "epoch 3181/5000,\t loss = 0.6175684928894043\n",
            "epoch 3191/5000,\t loss = 0.6173421144485474\n",
            "epoch 3201/5000,\t loss = 0.6171165704727173\n",
            "epoch 3211/5000,\t loss = 0.6168920397758484\n",
            "epoch 3221/5000,\t loss = 0.6166698932647705\n",
            "epoch 3231/5000,\t loss = 0.6164500713348389\n",
            "epoch 3241/5000,\t loss = 0.6162322759628296\n",
            "epoch 3251/5000,\t loss = 0.6160158514976501\n",
            "epoch 3261/5000,\t loss = 0.6157998442649841\n",
            "epoch 3271/5000,\t loss = 0.6155861020088196\n",
            "epoch 3281/5000,\t loss = 0.6153742074966431\n",
            "epoch 3291/5000,\t loss = 0.6151643395423889\n",
            "epoch 3301/5000,\t loss = 0.6149564385414124\n",
            "epoch 3311/5000,\t loss = 0.6147505640983582\n",
            "epoch 3321/5000,\t loss = 0.6145465970039368\n",
            "epoch 3331/5000,\t loss = 0.61434406042099\n",
            "epoch 3341/5000,\t loss = 0.6141434907913208\n",
            "epoch 3351/5000,\t loss = 0.6139447689056396\n",
            "epoch 3361/5000,\t loss = 0.6137471795082092\n",
            "epoch 3371/5000,\t loss = 0.6135498881340027\n",
            "epoch 3381/5000,\t loss = 0.613354504108429\n",
            "epoch 3391/5000,\t loss = 0.6131604909896851\n",
            "epoch 3401/5000,\t loss = 0.6129679679870605\n",
            "epoch 3411/5000,\t loss = 0.6127769947052002\n",
            "epoch 3421/5000,\t loss = 0.6125869154930115\n",
            "epoch 3431/5000,\t loss = 0.6123972535133362\n",
            "epoch 3441/5000,\t loss = 0.6122092008590698\n",
            "epoch 3451/5000,\t loss = 0.6120226383209229\n",
            "epoch 3461/5000,\t loss = 0.6118372678756714\n",
            "epoch 3471/5000,\t loss = 0.6116529107093811\n",
            "epoch 3481/5000,\t loss = 0.6114700436592102\n",
            "epoch 3491/5000,\t loss = 0.6112883687019348\n",
            "epoch 3501/5000,\t loss = 0.6111074090003967\n",
            "epoch 3511/5000,\t loss = 0.610927939414978\n",
            "epoch 3521/5000,\t loss = 0.610749363899231\n",
            "epoch 3531/5000,\t loss = 0.6105709671974182\n",
            "epoch 3541/5000,\t loss = 0.6103933453559875\n",
            "epoch 3551/5000,\t loss = 0.6102168560028076\n",
            "epoch 3561/5000,\t loss = 0.6100417375564575\n",
            "epoch 3571/5000,\t loss = 0.609868049621582\n",
            "epoch 3581/5000,\t loss = 0.6096954941749573\n",
            "epoch 3591/5000,\t loss = 0.6095238924026489\n",
            "epoch 3601/5000,\t loss = 0.6093534231185913\n",
            "epoch 3611/5000,\t loss = 0.6091840863227844\n",
            "epoch 3621/5000,\t loss = 0.609015941619873\n",
            "epoch 3631/5000,\t loss = 0.6088476777076721\n",
            "epoch 3641/5000,\t loss = 0.6086809039115906\n",
            "epoch 3651/5000,\t loss = 0.6085155010223389\n",
            "epoch 3661/5000,\t loss = 0.6083517074584961\n",
            "epoch 3671/5000,\t loss = 0.6081891059875488\n",
            "epoch 3681/5000,\t loss = 0.6080275177955627\n",
            "epoch 3691/5000,\t loss = 0.6078671813011169\n",
            "epoch 3701/5000,\t loss = 0.6077081561088562\n",
            "epoch 3711/5000,\t loss = 0.607550323009491\n",
            "epoch 3721/5000,\t loss = 0.6073936820030212\n",
            "epoch 3731/5000,\t loss = 0.6072383522987366\n",
            "epoch 3741/5000,\t loss = 0.6070842742919922\n",
            "epoch 3751/5000,\t loss = 0.6069309711456299\n",
            "epoch 3761/5000,\t loss = 0.6067787408828735\n",
            "epoch 3771/5000,\t loss = 0.6066274046897888\n",
            "epoch 3781/5000,\t loss = 0.606476902961731\n",
            "epoch 3791/5000,\t loss = 0.6063272953033447\n",
            "epoch 3801/5000,\t loss = 0.6061788201332092\n",
            "epoch 3811/5000,\t loss = 0.6060311794281006\n",
            "epoch 3821/5000,\t loss = 0.6058844327926636\n",
            "epoch 3831/5000,\t loss = 0.6057385206222534\n",
            "epoch 3841/5000,\t loss = 0.605593740940094\n",
            "epoch 3851/5000,\t loss = 0.6054495573043823\n",
            "epoch 3861/5000,\t loss = 0.6053066253662109\n",
            "epoch 3871/5000,\t loss = 0.6051642894744873\n",
            "epoch 3881/5000,\t loss = 0.6050230264663696\n",
            "epoch 3891/5000,\t loss = 0.6048824787139893\n",
            "epoch 3901/5000,\t loss = 0.604742705821991\n",
            "epoch 3911/5000,\t loss = 0.6046037077903748\n",
            "epoch 3921/5000,\t loss = 0.6044653654098511\n",
            "epoch 3931/5000,\t loss = 0.6043279767036438\n",
            "epoch 3941/5000,\t loss = 0.6041909456253052\n",
            "epoch 3951/5000,\t loss = 0.6040542125701904\n",
            "epoch 3961/5000,\t loss = 0.6039167046546936\n",
            "epoch 3971/5000,\t loss = 0.6037803292274475\n",
            "epoch 3981/5000,\t loss = 0.6036449670791626\n",
            "epoch 3991/5000,\t loss = 0.6035106778144836\n",
            "epoch 4001/5000,\t loss = 0.6033773422241211\n",
            "epoch 4011/5000,\t loss = 0.6032446622848511\n",
            "epoch 4021/5000,\t loss = 0.6031129360198975\n",
            "epoch 4031/5000,\t loss = 0.6029819250106812\n",
            "epoch 4041/5000,\t loss = 0.6028514504432678\n",
            "epoch 4051/5000,\t loss = 0.602721631526947\n",
            "epoch 4061/5000,\t loss = 0.6025928854942322\n",
            "epoch 4071/5000,\t loss = 0.6024648547172546\n",
            "epoch 4081/5000,\t loss = 0.602337658405304\n",
            "epoch 4091/5000,\t loss = 0.602211058139801\n",
            "epoch 4101/5000,\t loss = 0.602085292339325\n",
            "epoch 4111/5000,\t loss = 0.6019599437713623\n",
            "epoch 4121/5000,\t loss = 0.6018351912498474\n",
            "epoch 4131/5000,\t loss = 0.601711094379425\n",
            "epoch 4141/5000,\t loss = 0.6015876531600952\n",
            "epoch 4151/5000,\t loss = 0.6014649271965027\n",
            "epoch 4161/5000,\t loss = 0.6013426780700684\n",
            "epoch 4171/5000,\t loss = 0.6012210845947266\n",
            "epoch 4181/5000,\t loss = 0.6011002063751221\n",
            "epoch 4191/5000,\t loss = 0.6009799838066101\n",
            "epoch 4201/5000,\t loss = 0.6008606553077698\n",
            "epoch 4211/5000,\t loss = 0.6007419228553772\n",
            "epoch 4221/5000,\t loss = 0.6006240248680115\n",
            "epoch 4231/5000,\t loss = 0.600506603717804\n",
            "epoch 4241/5000,\t loss = 0.6003898978233337\n",
            "epoch 4251/5000,\t loss = 0.6002737879753113\n",
            "epoch 4261/5000,\t loss = 0.6001583933830261\n",
            "epoch 4271/5000,\t loss = 0.6000434160232544\n",
            "epoch 4281/5000,\t loss = 0.5999289751052856\n",
            "epoch 4291/5000,\t loss = 0.5998153686523438\n",
            "epoch 4301/5000,\t loss = 0.5997024178504944\n",
            "epoch 4311/5000,\t loss = 0.5995897650718689\n",
            "epoch 4321/5000,\t loss = 0.5994768142700195\n",
            "epoch 4331/5000,\t loss = 0.599364697933197\n",
            "epoch 4341/5000,\t loss = 0.5992527008056641\n",
            "epoch 4351/5000,\t loss = 0.5991408824920654\n",
            "epoch 4361/5000,\t loss = 0.5990299582481384\n",
            "epoch 4371/5000,\t loss = 0.5989198684692383\n",
            "epoch 4381/5000,\t loss = 0.5988104939460754\n",
            "epoch 4391/5000,\t loss = 0.598701536655426\n",
            "epoch 4401/5000,\t loss = 0.5985932946205139\n",
            "epoch 4411/5000,\t loss = 0.5984855890274048\n",
            "epoch 4421/5000,\t loss = 0.5983784794807434\n",
            "epoch 4431/5000,\t loss = 0.598271906375885\n",
            "epoch 4441/5000,\t loss = 0.5981658101081848\n",
            "epoch 4451/5000,\t loss = 0.5980603098869324\n",
            "epoch 4461/5000,\t loss = 0.5979551076889038\n",
            "epoch 4471/5000,\t loss = 0.5978504419326782\n",
            "epoch 4481/5000,\t loss = 0.5977462530136108\n",
            "epoch 4491/5000,\t loss = 0.5976425409317017\n",
            "epoch 4501/5000,\t loss = 0.5975394248962402\n",
            "epoch 4511/5000,\t loss = 0.5974363088607788\n",
            "epoch 4521/5000,\t loss = 0.5973336696624756\n",
            "epoch 4531/5000,\t loss = 0.5972316265106201\n",
            "epoch 4541/5000,\t loss = 0.5971303582191467\n",
            "epoch 4551/5000,\t loss = 0.5970293879508972\n",
            "epoch 4561/5000,\t loss = 0.5969290137290955\n",
            "epoch 4571/5000,\t loss = 0.5968289971351624\n",
            "epoch 4581/5000,\t loss = 0.5967294573783875\n",
            "epoch 4591/5000,\t loss = 0.5966302752494812\n",
            "epoch 4601/5000,\t loss = 0.5965316295623779\n",
            "epoch 4611/5000,\t loss = 0.5964334607124329\n",
            "epoch 4621/5000,\t loss = 0.5963353514671326\n",
            "epoch 4631/5000,\t loss = 0.5962372422218323\n",
            "epoch 4641/5000,\t loss = 0.5961397886276245\n",
            "epoch 4651/5000,\t loss = 0.5960427522659302\n",
            "epoch 4661/5000,\t loss = 0.5959434509277344\n",
            "epoch 4671/5000,\t loss = 0.5958452224731445\n",
            "epoch 4681/5000,\t loss = 0.5957481265068054\n",
            "epoch 4691/5000,\t loss = 0.5956503748893738\n",
            "epoch 4701/5000,\t loss = 0.5955519676208496\n",
            "epoch 4711/5000,\t loss = 0.5954539179801941\n",
            "epoch 4721/5000,\t loss = 0.5953571796417236\n",
            "epoch 4731/5000,\t loss = 0.5952614545822144\n",
            "epoch 4741/5000,\t loss = 0.5951664447784424\n",
            "epoch 4751/5000,\t loss = 0.5950722098350525\n",
            "epoch 4761/5000,\t loss = 0.5949785709381104\n",
            "epoch 4771/5000,\t loss = 0.594884991645813\n",
            "epoch 4781/5000,\t loss = 0.5947911739349365\n",
            "epoch 4791/5000,\t loss = 0.5946980714797974\n",
            "epoch 4801/5000,\t loss = 0.5946056842803955\n",
            "epoch 4811/5000,\t loss = 0.5945136547088623\n",
            "epoch 4821/5000,\t loss = 0.5944222807884216\n",
            "epoch 4831/5000,\t loss = 0.5943313837051392\n",
            "epoch 4841/5000,\t loss = 0.594238817691803\n",
            "epoch 4851/5000,\t loss = 0.5941439867019653\n",
            "epoch 4861/5000,\t loss = 0.5940500497817993\n",
            "epoch 4871/5000,\t loss = 0.5939565300941467\n",
            "epoch 4881/5000,\t loss = 0.5938635468482971\n",
            "epoch 4891/5000,\t loss = 0.5937702059745789\n",
            "epoch 4901/5000,\t loss = 0.5936769843101501\n",
            "epoch 4911/5000,\t loss = 0.593584418296814\n",
            "epoch 4921/5000,\t loss = 0.5934923887252808\n",
            "epoch 4931/5000,\t loss = 0.5934010148048401\n",
            "epoch 4941/5000,\t loss = 0.5933102965354919\n",
            "epoch 4951/5000,\t loss = 0.5932199358940125\n",
            "epoch 4961/5000,\t loss = 0.5931288003921509\n",
            "epoch 4971/5000,\t loss = 0.5930367112159729\n",
            "epoch 4981/5000,\t loss = 0.5929452776908875\n",
            "epoch 4991/5000,\t loss = 0.5928541421890259\n"
          ]
        }
      ],
      "source": [
        "model = FeedForward(2)\n",
        "history = learning_process(\n",
        "    model=model, \n",
        "    loss_function=F.cross_entropy, \n",
        "    lr=0.5,\n",
        "    n_epochs=5000,\n",
        "    accuracy=accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "DJ8c_0z3XZ2T",
        "outputId": "58a9bfbd-3975-487b-981e-dbb18f7246e1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy :  0.9833333333333333\n"
          ]
        }
      ],
      "source": [
        "show_learning(history)\n",
        "print(\"accuracy : \",accuracy(model(validation_inputs), validation_labels)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnFfcNOvVvhY"
      },
      "source": [
        "## Deviner le prochain mot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9yw-xpCVvhY"
      },
      "source": [
        "Dans cette section, nous allons construire un réseau de neurones multicouche qui apprend des mots en anglais. À ce fin, nous allons entraîner le réseau pour qu'il prend en entrée une sequence des (indices des) mots et donne en sortie une distribution de probabilité sur le mot qui vient après.  \n",
        "\n",
        "Il s'agit d'un modèle simple mais avec une multitude d'applications, comme la reconnaissance vocale, la correction automatique, la traduction de langues etc.. Pour votre curiosité vous trouverez une excellente introduction à ce modèle ici: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "Ces exercices sont une variante du mini-projet du cours d'introduction à l'apprentissage de [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) de l'Université de Toronto, dont vous trouverez une description ici: [CSC 321: Assignment 1](https://www.cs.toronto.edu/~yaojian/csc321/assignment1.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUX0uXSwVvhY"
      },
      "source": [
        "### Exercice 4 (création des grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8_g_GIhVvhZ"
      },
      "source": [
        "Téléchargez le fichier `raw_sentences.txt` et explorez-le avec un éditeur de texte. Le fichier contient des phrases qui seront utilisées pour faire apprendre notre modèle. Ces phrases sont particulièrement simples et basées sur un vocabulaire d'environ $250$ mots. Le première exercice consiste à extraire des séquences de 4 mots adjacents, appelées *4-grams*.\n",
        "\n",
        "Quelque convention que nous avons décidé d'adopter:\n",
        "- deux mots sur deux lignes différentes ne seront pas considérés comme adjacents;\n",
        "- nous ne faisons pas de différence entre lettres minuscules et majuscules;\n",
        "- nous prenons en considération la punctuation.\n",
        "\n",
        "Créer une liste `grams` des couples `(context, target)` telle que `context` est une liste de trois mots adjacents et `target` est le mot suivant à ces trois mots. Par exemple, les premières dix éléments de `grams` sont (dans notre implémentation): \n",
        "\n",
        "        [(['no', ',', 'he'], 'says'),\n",
        "        ([',', 'he', 'says'], 'now'),\n",
        "        (['he', 'says', 'now'], '.'),\n",
        "        (['and', 'what', 'did'], 'he'),\n",
        "        (['what', 'did', 'he'], 'do'),\n",
        "        (['did', 'he', 'do'], '?'),\n",
        "        (['the', 'money', \"'s\"], 'there'),\n",
        "        (['money', \"'s\", 'there'], '.'),\n",
        "        (['that', 'was', 'less'], 'than'),\n",
        "        (['was', 'less', 'than'], 'a')]\n",
        "\n",
        "Cette liste a une longueur totale de `465678` grams. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-XYL1RpQ-OLP"
      },
      "outputs": [],
      "source": [
        "def four_grams(filepath):\n",
        "  result = list()\n",
        "  with open(filepath, 'r') as file:\n",
        "    for line in file.readlines():\n",
        "      words = line.lower().split()\n",
        "      for i in range(3,len(words)):\n",
        "        result.append((words[i-3:i],words[i]))\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGT2sZoaVvhZ",
        "outputId": "edebc1e6-9b6c-4015-f31a-121ccc8c8b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(['no', ',', 'he'], 'says')\n",
            "([',', 'he', 'says'], 'now')\n",
            "(['he', 'says', 'now'], '.')\n",
            "(['and', 'what', 'did'], 'he')\n",
            "(['what', 'did', 'he'], 'do')\n",
            "(['did', 'he', 'do'], '?')\n",
            "(['the', 'money', \"'s\"], 'there')\n",
            "(['money', \"'s\", 'there'], '.')\n",
            "(['that', 'was', 'less'], 'than')\n",
            "(['was', 'less', 'than'], 'a')\n",
            "Longueur totale :  465686\n"
          ]
        }
      ],
      "source": [
        "# STUDENT, THIS IS YOUR JOB!\n",
        "sentences = four_grams('./drive/MyDrive/data/raw_sentences.txt')\n",
        "for gram in sentences[:10] : print(gram)\n",
        "\n",
        "print(\"Longueur totale : \",len(sentences))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkEnPXUMVvhZ"
      },
      "source": [
        "### Exercice 5 (transformation de `grams` en tenseurs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtynUnX3VvhZ"
      },
      "source": [
        "Les éléments `context` des couples des `grams` constituerons notre tenseur `inputs`, contenant les \"features\" du problème, alors que les éléments `target` donnent un tenseur `targets`, utilisé comme guide pour l'apprentissage du modèle. À cette fin, nous devons associer à chauqe mot un entier unique, appellé `idx`, qui nous permettra de travailler avec des tenseur de type `torch.long`. \n",
        "\n",
        "Créez le vocabulaire du jeu de données, cet-à dire l'ensemble des mots utilisés par `grams`. Il devrait avoir `249` mots. Créez deux dictionnaires `idx_from_word` et `word_from_idx` permettant d'associer un entier à un mot de façon unique (et vice-versa) en utilisant une énumération du vocabulaire. \n",
        "\n",
        "En utilisant ces dictionnaires et `grams` définissez donc les tenseurs `inputs` et `targets`. Ils devront avoir forme respectivement `[465678, 3]` et `[465678]`. \n",
        "\n",
        "Mélangez leur éléments en utilisant une même permutation pour les deux et départagez les deux tenseurs dans un \"training data-set\" et un \"validation data-set\" en proportion environ de 80% et 20% du jeu de données. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "3qrNd6KqVvha"
      },
      "outputs": [],
      "source": [
        "# STUDENT, THIS IS YOUR JOB\n",
        "def getDictionariesFrom(filepath):\n",
        "  idx_from_word = dict()\n",
        "  word_from_idx = dict()\n",
        "  id = 0\n",
        "  with open(filepath, 'r') as file:\n",
        "    for line in file.readlines():\n",
        "      words = line.lower().split()\n",
        "      for word in words:\n",
        "        if not word in word_from_idx :\n",
        "          word_from_idx[word] = id\n",
        "          idx_from_word[id] = word\n",
        "          id += 1\n",
        "  return idx_from_word, word_from_idx\n",
        "\n",
        "def toTensor(filepath, sentences):\n",
        "  _ ,  word_from_idx = getDictionariesFrom(filepath)\n",
        "  inputs = list()\n",
        "  targets = list()\n",
        "  for sentence in sentences : \n",
        "    input, target = sentence\n",
        "    inputs.append([word_from_idx[word] for word in input])\n",
        "    targets.append(word_from_idx[target])\n",
        "  return torch.tensor(inputs), torch.tensor(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcDT3c4Wf34G",
        "outputId": "24a7c5ae-5da5-4426-d65b-bccfd3f3a18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([465686, 3])\n",
            "torch.Size([465686])\n"
          ]
        }
      ],
      "source": [
        "inputs,  targets = toTensor('./drive/MyDrive/data/raw_sentences.txt', sentences)\n",
        "\n",
        "print(inputs.shape)\n",
        "print(targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "DOKSpu7AkEVM"
      },
      "outputs": [],
      "source": [
        "permutation = torch.randperm(len(inputs))\n",
        "inputs, targets = inputs[permutation], targets[permutation]\n",
        "# now I can split\n",
        "split = 8 * (len(inputs)) // 10 # 80% training, 20% validation\n",
        "train_inputs = inputs[:split]\n",
        "train_targets = targets[:split]\n",
        "validation_inputs = inputs[split:]\n",
        "validation_targets = targets[split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3680HngvVvha"
      },
      "source": [
        "### Exercice 6 (creation d'un data-loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mX_gT9ZVvha"
      },
      "source": [
        "Il est pratique d'avoir un \"training\" et un \"validation\" data-loader pour permettre d'expérimenter rapidement la descente du gradient avec des batch différents. Definissez donc une classe dédiée qui hérite du `Dataset` et utilisez la pour construire deux instances de `DataLoader`, une dédiée aux  \"training\", l'autre au \"validation\".\n",
        "\n",
        "(*Suggestion: n'hésitez pas à revoir les transparents du CMTP5 détaillant comment définir des nouveaux `DataLoader`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nahrJ-jIr19Q"
      },
      "outputs": [],
      "source": [
        "class MyDataSet(Dataset):\n",
        "  def __init__(self, inputs, targets):\n",
        "    self.inputs = inputs\n",
        "    self.labels = targets\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.inputs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.inputs[index], self.labels[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "TKidhOpMqHC1"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    MyDataSet(train_inputs, train_targets),\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "validation_loader = DataLoader(\n",
        "    MyDataSet(validation_inputs, validation_targets),\n",
        "    batch_size=128\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t2C82GtVvha"
      },
      "source": [
        "### Exercice 7 (definition du modèle multi-couche)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5qlrartVvhb"
      },
      "source": [
        "Vous devez définir un réseau de neurones qui se compose de $4$ couches:\n",
        "1. une couche d'entrée, qui prend en entrée trois `idx` de mots;\n",
        "2. une couche d'\"*immersion*\" qui donne une représentation interne du modèle des caractéristiques sémantiques des mots, en associant à chaque `idx` des mots un tenseur de flottants de dimension `d_dist_repr` (nous pouvons essayer `d_dist_repr = 8`, `d_dist_repr = 16` ou `d_dist_repr = 32`). Les neurones reliant la couche d'entrée à la couche d'\"*immersion*\" transforment donc le type `long` des `idx` au type `float`: vous pouvez le faire en utilisant le \"one-hot encoding\" des `idx` et ensuite un module `nn.Linear`, ou bien en utilisant directement le module `nn.Embedding` de PyTorch (voir API). Il est importante que les trois `idx` soient transformés en utilisant **le même ensemble de paramètres** pour mapper chaque indice de la couche d'entrée dans la couche d'\"*immersion*\";  \n",
        "3. une couche cachée qui est une couche linéaire de dimension `d_hidden` (nous essayerons par exemple `d_hidden = 64` et `d_hidden = 256`) avec activation à votre choix; \n",
        "4. une couche de sortie, qui donne une distribution de probabilité sur les mots du vocabulaire, donnant le mot suivant du gram passé en entrée. Vous pouvez considérer deux possibles fonctions d'activation: `softmax` et `log_softmax`. (Attention, vous allez voir dans l'apprentissage qu'une des deux activations marche nettement mieux que l'autre, savez vous dire pour quelle raison ?).\n",
        "\n",
        "La création du réseau doit être paramétrisé par `n_words` (les nombres des mots dans le vocabulaire), `d_context` (le nombre de mots dans un échantillon de `inputs`, dans notre cas $3$), `d_dist_repr` (la dimension de la couche d'\"*immersion*\") et par `d_hidden` (la dimension de la couche interne au modèle). \n",
        "\n",
        "(*Attention, une spécificité de ce modèle par rapport à l'architecture feedforward standard est que la couche d'\"*immersion*\" est calculée avec la même transformation linéaire sur les trois indices d'entrée. En effet, la couche interne aurait `d_context * d_dist_repr` neurones en entrée et sortira avec `d_hidden` connections. À cette fin, il faudrait modifier la forme de la sortie du premier module de neurones, en ayant `d_context` et `d_dist_repr` sur deux axes distinctes.*)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fyLaDgTVvhb",
        "outputId": "5df96cc5-69a8-457e-88d5-57ca228901e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-5.4196, -5.7149, -5.3213,  ..., -5.7261, -5.6351, -5.7329],\n",
              "        [-5.2567, -5.7961, -4.9743,  ..., -5.4116, -5.5494, -5.3992],\n",
              "        [-5.5418, -5.8964, -5.3482,  ..., -5.9446, -5.7742, -5.7137],\n",
              "        ...,\n",
              "        [-5.4237, -5.7937, -5.3649,  ..., -5.6343, -5.4481, -5.5192],\n",
              "        [-5.4641, -5.6905, -5.2917,  ..., -5.3718, -5.3272, -5.3432],\n",
              "        [-5.4370, -5.8311, -5.3160,  ..., -5.6589, -5.4821, -5.3336]],\n",
              "       grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# STUDENT, THIS IS YOUR JOB\n",
        "class WordPredict(nn.Module):\n",
        "  \"\"\"\n",
        "    n_words : les nombres des mots dans le vocabulaire\n",
        "    d_context : le nombre de mots dans un échantillon de inputs, dans notre cas  3\n",
        "    d_dist_repr : la dimension de la couche d'\"immersion\"\n",
        "    d_hidden : la dimension de la couche interne au modèle\n",
        "  \"\"\"\n",
        "  def __init__(self,n_words,d_context,d_dist_repr,d_hidden):\n",
        "    super(WordPredict, self).__init__()                     \n",
        "    self.embadding = nn.Embedding(n_words, d_dist_repr)   # trois idx soient transformés en utilisant le même ensemble de paramètres\n",
        "    self.fct1 = Linear(d_dist_repr, d_hidden, bias=True)\n",
        "    self.fct2 = Linear(d_hidden, n_words, bias=True)  \n",
        "\n",
        "  def forward(self, input):\n",
        "    emb          = self.embadding(input)\n",
        "    layer1       = F.relu(torch.flatten(emb,end_dim=1))\n",
        "    hidden_layer = F.relu(self.fct1(layer1))\n",
        "    output       = F.log_softmax(self.fct2(hidden_layer),dim=1)\n",
        "    return output \n",
        "\n",
        "model = WordPredict(250,3,16,48)\n",
        "model(train_inputs[0:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPC2jRfnVvhb"
      },
      "source": [
        "### Exercice 8 (apprentissage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVUI2e8IVvhb"
      },
      "source": [
        "Faites apprendre votre modèle en utilisant l'optimiseur `Adam` et comme fonction de perte `nn.CrossEntropyLoss` ou `nn.NLLoss` (voir API), selon la fonction d'activation choisie pour la couche de sortie. Pour peaufiner les hyperparametres de votre apprentissage il faut choisir une notion de taux d'exactitude (ou d'erreur). En effet il ne s'agit pas vraiment d'un problème de classification, mais plutôt d'un problème d'estimation d'une distribution probabiliste. Dans ce cas vous pouvez utiliser la même fonction de perte (mais sur le \"validation data-set\") pour évaluer la performance de vos hyperparametres. \n",
        "\n",
        "Trouvez un bon taux d'apprentissage (*learning rate*) et nombre d'époques. Combien de paramètres a votre modèle ? Avec cette dimension l'utilisation des GPU deviennent utile, voir indispensable. Malheureusement, la disponibilité  de la mémoire RAM pour les comptes gratuits de colab peut être limitative. La quantité de RAM de colab accessible varie selon le nombre d'utilisateurs... d'après mon expérience, tôt le matin (par exemple vers 7 heures), colab vous offre pas mal des ressources. Si par contre vous n'aimez pas trop vous levez tôt, vous pouvez vous rappeler que faire des batches aide à limiter beaucoup l'utilisation de la RAM... \n",
        "\n",
        "Vous pouvez aussi expérimenter et choisir entre différentes variants d'algorithme de la descente du gradient, comme le \"gradient descent with batches\" ou le \"stochastic mini-batch gradient descent\" (voir transparents CMTP5). \n",
        "\n",
        "Vous pouvez aussi sauvegarder et télécharger les paramètres du modèle après l'apprentissage (voir [API](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-and-loading-models)) pour le tester directement sur votre machine. Attention, l'accès aux GPU des colab est instable et il peut s'interrompre après un délais d'attente trop longue ou le dépassement des limites d'utilisation de la mémoire. Nous vous conseillons donc de télécharger les paramètres de votre modèle toujours sur votre machine locale. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "y9MivP5uVvhb"
      },
      "outputs": [],
      "source": [
        "def train_loop(train_loader, model, loss_map,lr=0.06,epochs=15):\n",
        "    # use gpu if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    model.to(device)\n",
        "    \n",
        "    # create optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history_loss = []\n",
        "\n",
        "    # Train model\n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0.\n",
        "        for images, labels in train_loader:\n",
        "            # Transfers data to GPU\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # Primal computation\n",
        "            output = model(images)            \n",
        "            loss = loss_map(output, labels)            \n",
        "            # Gradient computation\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            # perform parameter update based on current gradients\n",
        "            optimizer.step()\n",
        "            # compute the epoch training loss\n",
        "            loss_epoch += loss.item()\n",
        "            \n",
        "        # display the epoch training loss\n",
        "        print(f\"epoch : {epoch + 1}/{epochs}, loss = {loss_epoch:.6f}\")  \n",
        "        history_loss.append(loss_epoch)\n",
        "    return history_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErV5zRB7Vvhc"
      },
      "source": [
        "Une fois que votre modèle a terminé son apprentissage, vous pouvez l'exploiter pour programmer des fonctions sur votre vocabulaire. Programmez les fonctions suivantes, en utilisant le modèle en entier ou le bloc `nn.Embedding` du modèle.\n",
        "- `predict_next_word(context, model)` : étant donné un triplet `context` des trois mots du vocabulaire, la fonction renvoie la mot la plus probable selon `model` qui suit ce triplet.\n",
        "- `word_distance(w1, w2, model)` : étant données deux mots `w1` et `w2` du vocabulaire, la fonction renvoie la distance relative des représentations \"immersion\" (c.à-d. de la sortie du module `Embedding` du `model`) de cex deux mots. \n",
        "- `display_nearest_words(w, model, top)` étant donné un entier `top`, la fonction renvoie les `top` mots du vocabulaire les plus proches au mot `w` come calculé par `word_distance`.\n",
        "\n",
        "Donnez des exemples d'utilisation de chacune de ces fonctions et n'hésitez pas à commenter leur résultat. En particulier, avez vous une intuition de quel type de distance mesure `word_distance`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "41zDexopVvhc"
      },
      "outputs": [],
      "source": [
        "# STUDENT, THIS IS YOUR JOB!\n",
        "idx_from_word, word_from_idx = getDictionariesFrom('./drive/MyDrive/data/raw_sentences.txt')\n",
        "\n",
        "def predict_next_word(context, model):\n",
        "  context_idx = torch.tensor([\n",
        "    word_from_idx[context[0]],\n",
        "    word_from_idx[context[1]],\n",
        "    word_from_idx[context[2]],                           \n",
        "  ])\n",
        "  prediction = torch.argmax(model(context_idx))\n",
        "  return idx_from_word[prediction.item()]\n",
        "\n",
        "def word_distance(w1, w2, model):\n",
        "  w1_idx = torch.tensor(word_from_idx[w1])\n",
        "  w2_idx = torch.tensor(word_from_idx[w2])\n",
        "  a = model.embadding(w1_idx)\n",
        "  b = model.embadding(w2_idx)\n",
        "  return torch.sqrt(sum( (b-a)**2 ))\n",
        "\n",
        "def display_nearest_words(w, model, top):\n",
        "  distances = [ (idx,word_distance(w,w2,model)) for idx,w2 in idx_from_word.items()]\n",
        "  distances = sorted(distances, key = lambda x: x[1])\n",
        "  return torch.tensor([ pos for pos,_ in distances[1:top+1]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copie de TP6_Moodle.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "db229b0a4cfa2fcf23e8658bb546a4e9861ee4414145c0b3a3fc9e7ec2682689"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
